\documentclass[a4paper, 12pt]{article}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{titlesec}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\pgfplotsset{width=12cm, compat=1.18}
\newgeometry{left=1.5 cm, right=1.5cm, top=1.5cm, bottom=1.5cm}

\begin{document}


% ---------------------------------- Титульник ----------------------------------
\hypersetup{pageanchor=false}
\begin{titlepage}
 \begin{center}
  \vspace*{1cm}

  \Huge
  \textbf{Лабораторная работа №6}

  \vspace{0.5cm}
  \LARGE
  ``Коллективные операции в MPI''

  \vspace{1.5cm}
  Выполнил студент группы Б20-505\\
  \textbf{Сорочан Илья}

  \vfill

  \Large
  Московский Инженерно-Физический Интститут\\
  Москва 2023

 \end{center}
\end{titlepage}


% ---------------------------------- Рабочая среда ----------------------------------

\section{Рабочая среда}

Технические характеристики (вывод \textit{inxi}):
\begin{verbatim}
CPU: 6-core AMD Ryzen 5 4500U with Radeon Graphics (-MCP-)
speed/min/max: 1396/1400/2375 MHz Kernel: 5.15.85-1-MANJARO x86_64 Up: 46m
Mem: 2689.5/7303.9 MiB (36.8%) Storage: 238.47 GiB (12.6% used) Procs: 238
Shell: Zsh inxi: 3.3.24
\end{verbatim}

Используемый компилятор:
\begin{verbatim}
gcc (GCC) 12.2.0
\end{verbatim}

Версия MPI:
\begin{verbatim}
Open MPI 4.1.4
\end{verbatim}

Согласно \href{https://www.openmp.org/resources/openmp-compilers-tools/}{официальной документации} даная версия компилятора поддерживает \textit{OpenMP 5.0} (необходимо для сравнения с первой лабораторной)


% ---------------------------------- Сортировка Шелла ----------------------------------

\section{Реализация сортирвки Шелла с использованием \textit{MPI}}

Решить данную задачу я решил следующим образом:
\begin{enumerate}
 \item Инициализация MPI и прочих необходимых значений;
 \item Распределение частей массива между процессами;
 \item Сортировка шеллом каждого фрагмента;
 \item Объединение всех фрагментов;
\end{enumerate}

Если с инициализацией и сортировкой все приблизительно понятно, то с другими этапами нет. Их я уточну далее.

\subsection{Распределение массива}

Для того что бы распределить массив равномерно между процессами был использовал \textit{MPI\_Scatterv}. Она учитывает случаи, когда наш массив не делится на число процессов ровно, однако сложнее в применении.

После распределения фрагментов вызывается сортировка.

После сортировки фрагменты собираются во временный массив \textit{tmp}. Он будет использоваться на следующем этапе. Непосредственно для сборки применялся \textit{MPI\_Gatherv}.

\subsection{Объединение фрагментов}

Теперь все фрагменты объеденины в одном массиве, длинна которого равна начальному. Каждый фрагмент отсортирован, поэтому достаточно брать элементы с их ``верхов``.

Тоесть по сути это одна итерация сортировки слиянием.

Из-за того, что число фрагментов является так же числом процессов, их тяжело объединять параллельно. Если каждому процессу назначить два фрагмента, то половина всех процессов будет простаивать.

Может показатся, что в качестве решения стоит делить каждый фрагмент пополам, однако таким образом фрагменты, которые нужно объединить лишь множатся.

Поэтому мной было принято решение произвести слияние в главном процессе.

\subsection{Исходный код}

Код программы:

\lstinputlisting[language=C, basicstyle=\scriptsize]{src/main.c}
\vspace{0.5cm}

Дополнительный скрипт:

\lstinputlisting[language=Python, basicstyle=\scriptsize]{src/main.py}
\vspace{0.5cm}

% ---------------------------------- Графики ----------------------------------

\section{Экспериментальные данные}

Во всех измерениях бралось 10 запусков на поток.

\vspace{0.3cm}

\begin{tikzpicture}
 \begin{axis}[
    xlabel={Число процессов},
    ylabel={Время (мс)},
    legend pos=north east,
  ]
  \addplot table [x=Threads, y=Worst (ms), col sep=comma] {data/mpi_data.csv};
  \addplot table [x=Threads, y=Best (ms), col sep=comma] {data/mpi_data.csv};
  \addplot table [x=Threads, y=Average (ms), col sep=comma] {data/mpi_data.csv};
  \legend{Худшее время, Лучшее время, Среднее время}
 \end{axis}
\end{tikzpicture}

\vspace{0.5cm}

Из графика видно, что \textit{MPI} достигает ускорения за счет хорошей производительности в худших случаях. Лучшее время у них почти одинаково.
Взглянем на графики с оптимизациями:

\vspace{0.3cm}

\begin{tikzpicture}
 \begin{axis}[
    xlabel={Число процессов},
    ylabel={Среднее время (мс)},
    legend pos=north east,
  ]
  \addplot table [x=Threads, y=Worst (ms), col sep=comma] {data/mpi_optimize.csv};
  \addplot table [x=Threads, y=Best (ms), col sep=comma] {data/mpi_optimize.csv};
  \addplot table [x=Threads, y=Average (ms), col sep=comma] {data/mpi_optimize.csv};
  \legend{Худшее время, Лучшее время, Среднее время}
 \end{axis}
\end{tikzpicture}

\vspace{0.3cm}

Хочу обратить внимание, что с оптимизациями \textit{MPI} проигрывает однопоточному запуску. Возможно алгоритм все еще слишком прост, как было в предыдущих лабораторных.

Рассмотрим прирост производительности (среднее время; без оптимизаций):

\begin{tikzpicture}
 \begin{axis}[
    xlabel={Число процессов},
    ylabel={Прирост (\%)},
    ybar interval=0.7,
  ]
  \addplot table [x index=0, y index=1, col sep=comma] {data/mpi_compare.csv};
 \end{axis}
\end{tikzpicture}


% ---------------------------------- Сравнение ----------------------------------

\section{Сравнение с \textit{OpenMP}}

Сравним \textit{MPI} и \textit{OpenMP}:

\vspace{0.3cm}

\begin{tikzpicture}
 \begin{axis}[
    xlabel={Число процессов/потоков},
    ylabel={Среднее время (мс)},
    legend pos=north east,
  ]
  \addplot table [x=Threads, y=Average (ms), col sep=comma] {data/mpi_data.csv};
  \addplot table [x=Threads, y=Average (ms), col sep=comma] {data/data.csv};
  \legend{\textit{MPI}, \textit{OpenMP}}
 \end{axis}
\end{tikzpicture}

\vspace{0.3cm}

\textit{MPI} выигрывает с большим отрывом и при этом, практически не теряет в производительности при увеличении числа процессов. Хоть разработка программы на \textit{MPI} была сложнее и заняла больше времени она оказалась производительнее.

Самое удивительное это то, что ситуция не меняется с применением оптимизаций:

\begin{tikzpicture}
 \begin{axis}[
    xlabel={Число процессов/потоков},
    ylabel={Среднее время (мс)},
    legend pos=north east,
  ]
  \addplot table [x=Threads, y=Average (ms), col sep=comma] {data/mpi_optimize.csv};
  \addplot table [x=Threads, y=Average (ms), col sep=comma] {data/optimized.csv};
  \legend{\textit{MPI}, \textit{OpenMP}}
 \end{axis}
\end{tikzpicture}


% ---------------------------------- Заключение ----------------------------------

\section{Заключение}
В данной работе была разработана параллельноя версия сортировки Шелла с использованием \textit{MPI}. Был написан специальный скрипт для сбора данных. Осуществлено сравнение скорости исполнения версии программы использующей \textit{MPI} и её аналогом из третьей лабораторной работы, использующий \textit{OpenMP}.

В ходе работы было выяснено, что разработка с использованием \textit{MPI} занимает гораздо больше времени, но является эффективнее. Автор так же допускает возможность неэффективной параллелизации - что еще может повысить результаты \textit{MPI}.

С другой стороны следует отметить, что в третьей лабораторной работе не устанавливалось распределение нагрузки (флаг \textit{schedule}), что может подкорректировать результаты \textit{OpenMP} в лучшую сторону.


\end{document}
